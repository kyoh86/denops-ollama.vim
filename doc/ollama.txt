*ollama.txt* A Vim plugin as to chat with Ollama.
                                                  *ollama* *denops-ollama.txt*

Author: kyoh86 <me@kyoh86.dev>
License: MIT License


==============================================================================
                                                             *ollama-contents*
Contents ~

Install						|ollama-install|
Function					|ollama-function|
Commands					|ollama-commands|
Keymaps						|ollama-keymaps|
Denops commands	 				|ollama-denops-commands|
Other references				|ollama-references|


==============================================================================
                                                              *ollama-install*
Install ~

You need to install |denops.vim| as a dependency.

* vim-denops/denops.vim https://github.com/vim-denops/denops.vim
* ollama https://ollama.ai

If you want to use it, run ollama background with a model you like in local.

```console
$ ollama pull codellama
$ ollama serve
```

And call `ollama#start_chat` with the model name.


==============================================================================
                                                             *ollama-function*
Function ~

                                                         *ollama#open_log()*
ollama#open_log([{options}])
	Open Ollama log file.

	You can specify {options} with fields in below.
		opener	How the new buffer be placed in which window:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window


	Example: >
	:call ollama#open_log({'opener': 'new'})
<

                                                         *ollama#start_chat()*
ollama#start_chat({model} [, {options} [, {params}]])
	Start chat with Ollama using the {model}.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		opener		How the new buffer be placed in which window:
				tabnew		(DEFAULT) open a new tab
				split, new	open a new split
				vsplit, vnew	open a new vertical split
				edit		open in the current window
		timeout		Time limit.
		baseUrl		The base of the URL to request.
				(DEFAULT): "http://localhost:11434"
		initialPrompt	Prompt to be passed to Ollama. When this is
				specified, Ollama takes it immediately and
				begins its reply.

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example 1: Start a chat with the local Ollama server with
	             "codellama" model. >
	call ollama#start_chat('codellama')
<
	Example 2: Start a new chat with the remote Ollama server. >
	call ollama#start_chat('codellama',
		\ {'baseUrl': 'http://your-remote-server'})
<
	Example 3: Start a chat with a initial prompt. >
	call ollama#start_chat('codellama', 
		\ { 'initialPrompt': 'What is your favorite color?' })
<

                                            *ollama#start_chat_with_context()*
ollama#start_chat_with_context({model}, {context} [, {options} [, {params}]])
	Start chat with Ollama with the context using the {model}.
	See |ollama-api-definition|.

	{context} specifies what kind of context should be passed to Ollama
	in advance.
	It supports the following entries:
		headMessage	A first message to be sent.
		selection	If the selection be sent.
		currentBuffer	If the current buffer be sent.
		buffers		A list of the bufinfo.
				It accepts buffer numbers (see |bufnr()|) or
			      	the objects from |getbufinro()|.
		lastMessage	A last message to be sent.

	You can specify {options} with fields in below.
		opener		How the new buffer be placed in which window:
				tabnew		(DEFAULT) open a new tab
				split, new	open a new split
				vsplit, vnew	open a new vertical split
				edit		open in the current window
		timeout		Time limit.
		baseUrl		The base of the URL to request.
				(DEFAULT): "http://localhost:11434"
		initialPrompt	Prompt to be passed to Ollama. When this is
				specified, Ollama takes it immediately and
				begins its reply.

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example 1: Start a new chat based on the selection text. >
	call ollama#start_chat('codellama',
		\ {'selection': v:true})
<
	Example 2: Start a new chat on the current buffer. >
	call ollama#start_chat('codellama',
		\ {'currentBuffer': v:true})
<
	Example 3: Start a new chat on the listed buffers. >
	:call ollama#start_chat_with_context('codellama', 
		\ {'buffers:' getbufinfo({'buflisted':v:true})})
<
                                                           *ollama#complete()*
ollama#complete({model}, {callback} [, {options} [, {params}]])
	Get completion for the current buffer around the cursor by the {model}.
	See |ollama-api-definition|.

	Response should be pass to {callback}.

	You can specify {options} with fields in below.
		timeout		Time limit.
		baseUrl		The base of the URL to request.
				(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: Request completion and echo it. >
	:call ollama#complete('codellama', {msg -> execute("echomsg " .. msg)})
<

                                                        *ollama#list_models()*
ollama#list_models([{options}])
	Show list models in local.

	You can specify {options} with fields in below.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	Example: >
	:call ollama#list_models()
<

                                                         *ollama#pull_model()*
ollama#pull_model({name} [, {options} [, {params}]])
	Pull a model by the {name} from the library.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		insecure	If it is true, allow insecure connections to
				the library.
		timeout		Time limit.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: >
	:call ollama#pull_model('codellama')
<

                                                       *ollama#delete_model()*
ollama#delete_model({name} [, {options}])
	Delete a model by the {name} in local.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	Example: >
	:call ollama#delete_model('codellama')
<


==============================================================================
                                                             *ollama-commands*
Commands ~


==============================================================================
                                                              *ollama-keymaps*
Keymaps ~


==============================================================================
                                                       *ollama-common-options*
Common options ~



==============================================================================
                                                           *ollama-references*
Other references ~

                                                       *ollama-api-definition*
API definitions ~
	See https://github.com/jmorganca/ollama/blob/main/docs/api.md


==============================================================================
" vim:tw=78:ts=8:sw=8:ft=help:norl:noet:fen:
