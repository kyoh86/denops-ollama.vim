*ollama.txt* A Vim plugin as to chat with Ollama.
                                                  *ollama* *denops-ollama.txt*

Author: kyoh86 <me@kyoh86.dev>
License: MIT License


==============================================================================
                                                             *ollama-contents*
Contents ~

Install						|ollama-install|
Customize					|ollama-custom|
Function					|ollama-function|
Commands					|ollama-commands|
Keymaps						|ollama-keymaps|
Denops commands	 				|ollama-denops-commands|
Other references				|ollama-references|


==============================================================================
                                                              *ollama-install*
Install ~

You need to install |denops.vim| and ollama.ai as a dependency.

* vim-denops/denops.vim https://github.com/vim-denops/denops.vim
* ollama https://ollama.ai

If you want to use it, run ollama background with a model you like in local. >
	$ ollama pull codellama
	$ ollama serve
<

Ollama provides systemd unit for Linux, so if you are using it in Linux, you 
can start it with `systemctl`. >
	$ systemctl start ollama
<

Then, you can call `ollama#start_chat` with the model name.

==============================================================================
                                                               *ollama-custom*
Customize ~

This plugin only provides some vim functions like |ollama#start_chat()| that
have simple and flexible interface. But also they are not convenience to use
naively for each time. So we may pre-set arguments for each functions with
|ollama#custom#set_func_arg| and |ollama#custom#patch_args|.

For example, we can set "model" argument of the |ollama#start_chat()|: >
	call ollama#custom#set_func_arg("startChat", "model", "codellama")
<
Then, we can call it without the argument: >
	call ollama#start_chat({})
<
It is equivalent to calling it like this: >
	call ollama#start_chat({"model": "codellama"})
<

==============================================================================
                                                             *ollama-function*
Function ~

                                                           *ollama#open_log()*
ollama#open_log({args})
	Open Ollama log file.

	{args} is a dictionary having fields in below.
	- opener	(Optional) How the new window be placed:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window


	Example 1: Open log >
	:call ollama#open_log({})
<

	Example 2: Open log in new window (split horizontal)>
	:call ollama#open_log({'opener': 'new'})
<

                                                         *ollama#start_chat()*
ollama#start_chat({args})
	Start chat with Ollama

	{args} is a dictionary having fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- opener	(Optional) How the new window be placed:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")
	- message	(Optional) Prompt to be passed to Ollama.
			When this is specified, Ollama takes it
			immediately and begins its reply.
	- images	(Optional) A list of base64-encoded images
			(for multimodal models such as llava)
	- options	(Optional) Additional model parameters listed in the
			documentation for the Modelfile such as temperature
	- system	(Optional) System message to (overrides what is
			defined in the Modelfile)

	Example 1: Start a chat with the local Ollama server with
	             "codellama" model. >
	call ollama#start_chat({'model': 'codellama'})
<
	Example 2: Start a new chat with the remote Ollama server. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'baseUrl': 'http://your-remote-server'})
<
	Example 3: Start a chat with a initial prompt. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'message': 'What is your favorite color?'})
<

                                                  *ollama#start_chat_in_ctx()*
ollama#start_chat_in_ctx({args})
	Start chat with Ollama with the context.

	{args} is a dictionary having fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- context	Specifies what kind of context should be passed to
			Ollama in advance. It supports the following entries:
			- headMessage	A first message to be sent.
			- selection	If the selection be sent.
			- currentBuffer	If the current buffer be sent.
			- buffers	A list of the bufinfo. It accepts
					buffer numbers (see |bufnr()|) or
					the objects from |getbufinro()|.
			- lastMessage	A last message to be sent.
	- opener	(Optional) How the new window be placed:
			tabnew		(Default) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")
	- message	(Optional) Prompt to be passed to Ollama. When this is
			specified, Ollama takes it immediately and
			begins its reply.
	- options	(Optional) Additional model parameters listed in the
			documentation for the Modelfile such as temperature

	Example 1: Start a new chat based on the selection text. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'context': {'selection': v:true}})
<
	Example 2: Start a new chat on the current buffer. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'context': {'currentBuffer': v:true}})
<
	Example 3: Start a new chat on the listed buffers. >
	:call ollama#start_chat_in_ctx({
		\ 'model': 'codellama', 
		\ 'context': {'buffers:' getbufinfo({'buflisted':v:true})}})
<
                                                           *ollama#complete()*
ollama#complete({args})
	Get completion for the current buffer around the cursor.

	{args} is a dictionary having fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- callback	A function should take response.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")
	- images	(Optional) A list of base64-encoded images
			(for multimodal models such as llava)
	- options	(Optional) Additional model parameters listed in the
			documentation for the Modelfile such as temperature
	- system	(Optional) System message to (overrides what is
			defined in the Modelfile)

	Example: Request completion and echo it. >
	:call ollama#complete({
		\ 'model': 'codellama',
		\ 'callback': {msg -> execute("echomsg " .. msg)}})
<

                                                        *ollama#list_models()*
ollama#list_models({args})
	Show list models in local.

	{args} is a dictionary having fields in below.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	Example: >
	:call ollama#list_models({})
<

                                                         *ollama#pull_model()*
ollama#pull_model({args})
	Pull a model from the library.

	{args} is a dictionary having fields in below.
	- name		Ollama model name. |ollama-api-definition|
	- insecure	(Optional) If it is true, allow insecure connections
			to the library.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	Example: >
	:call ollama#pull_model({'model': 'codellama'})
<

                                                       *ollama#delete_model()*
ollama#delete_model({args})
	Delete a model in local.

	{args} is a dictionary having fields in below.
	- name		Ollama model name. |ollama-api-definition|
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	Example: >
	:call ollama#delete_model({'model': 'codellama'})
<


==============================================================================
                                                             *ollama-commands*
Commands ~


==============================================================================
                                                              *ollama-keymaps*
Keymaps ~


==============================================================================
                                                       *ollama-common-options*
Common options ~



==============================================================================
                                                           *ollama-references*
Other references ~

                                                       *ollama-api-definition*
API definitions ~
	See https://github.com/jmorganca/ollama/blob/main/docs/api.md


==============================================================================
" vim:tw=78:ts=8:sw=8:ft=help:norl:noet:fen:
