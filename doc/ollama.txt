*ollama.txt* A Vim plugin as to chat with Ollama.
                                                  *ollama* *denops-ollama.txt*

Author: kyoh86 <me@kyoh86.dev>
License: MIT License


==============================================================================
                                                             *ollama-contents*
Contents ~

Install						|ollama-install|
Function					|ollama-function|
Commands					|ollama-commands|
Keymaps						|ollama-keymaps|
Denops commands	 				|ollama-denops-commands|
Other references				|ollama-references|


==============================================================================
                                                              *ollama-install*
Install ~

You need to install |denops.vim| as a dependency.

* vim-denops/denops.vim https://github.com/vim-denops/denops.vim
* ollama https://ollama.ai

If you want to use it, run ollama background with a model you like in local.

```console
$ ollama pull codellama
$ ollama serve
```

And call `ollama#start_chat` with the model name.


==============================================================================
                                                             *ollama-function*
Function ~

                                                         *ollama#open_log()*
ollama#open_log({opts})
	Open Ollama log file.

	{opts} has fields in below.
	- opener	(Optional) How the new window be placed:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window


	Example 1: Open log >
	:call ollama#open_log({})
<

	Example 2: Open log in new window (split horizontal)>
	:call ollama#open_log({'opener': 'new'})
<

                                                         *ollama#start_chat()*
ollama#start_chat({opts} [, {params}])
	Start chat with Ollama

	{opts} has fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- opener	(Optional) How the new window be placed:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")
	- initialPrompt	(Optional) Prompt to be passed to Ollama.
			When this is specified, Ollama takes it
			immediately and begins its reply.

	And you can specify {params} as optional parameters for the API.
	See |ollama-api-definition|.

	Example 1: Start a chat with the local Ollama server with
	             "codellama" model. >
	call ollama#start_chat({'model': 'codellama'})
<
	Example 2: Start a new chat with the remote Ollama server. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'baseUrl': 'http://your-remote-server'})
<
	Example 3: Start a chat with a initial prompt. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'initialPrompt': 'What is your favorite color?'})
<

                                            *ollama#start_chat_with_context()*
ollama#start_chat_with_context({opts} [, {params}])
	Start chat with Ollama with the context.

	{opts} has fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- context	Specifies what kind of context should be passed to
			Ollama in advance. It supports the following entries:
			- headMessage	A first message to be sent.
			- selection	If the selection be sent.
			- currentBuffer	If the current buffer be sent.
			- buffers	A list of the bufinfo. It accepts
					buffer numbers (see |bufnr()|) or
					the objects from |getbufinro()|.
			- lastMessage	A last message to be sent.
	- opener	(Optional) How the new window be placed:
			tabnew		(Default) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")
- initialPrompt	Prompt to be passed to Ollama. When this is
			specified, Ollama takes it immediately and
			begins its reply.

	And you can specify {params} as optional parameters for the API.
	See |ollama-api-definition|.

	Example 1: Start a new chat based on the selection text. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'context': {'selection': v:true}})
<
	Example 2: Start a new chat on the current buffer. >
	call ollama#start_chat({
		\ 'model': 'codellama',
		\ 'context': {'currentBuffer': v:true}})
<
	Example 3: Start a new chat on the listed buffers. >
	:call ollama#start_chat_with_context({
		\ 'model': 'codellama', 
		\ 'context': {'buffers:' getbufinfo({'buflisted':v:true})}})
<
                                                           *ollama#complete()*
ollama#complete({opts} [, {params}])
	Get completion for the current buffer around the cursor.

	{opts} has fields in below.
	- model		Ollama model name. |ollama-api-definition|
	- callback	A function should take response.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	And you can specify {params} as optional parameters for the API.
	See |ollama-api-definition|.

	Example: Request completion and echo it. >
	:call ollama#complete({
		\ 'model': 'codellama',
		\ 'callback': {msg -> execute("echomsg " .. msg)}})
<

                                                        *ollama#list_models()*
ollama#list_models({opts})
	Show list models in local.

	{opts} has fields in below.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	Example: >
	:call ollama#list_models({})
<

                                                         *ollama#pull_model()*
ollama#pull_model({name} [, {opts} [, {params}]])
	Pull a model from the library.

	{opts} has fields in below.
	- name		Ollama model name. |ollama-api-definition|
	- insecure	(Optional) If it is true, allow insecure connections
			to the library.
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	And you can specify {params} as optional parameters for the API.
	See |ollama-api-definition|.

	Example: >
	:call ollama#pull_model({'model': 'codellama'})
<

                                                       *ollama#delete_model()*
ollama#delete_model({name} [, {opts}])
	Delete a model in local.

	{opts} has fields in below.
	- name		Ollama model name. |ollama-api-definition|
	- timeout	(Optional) Time limit.
	- baseUrl	(Optional) The base of the URL to request.
			(Default: "http://localhost:11434")

	Example: >
	:call ollama#delete_model({'model': 'codellama'})
<


==============================================================================
                                                             *ollama-commands*
Commands ~


==============================================================================
                                                              *ollama-keymaps*
Keymaps ~


==============================================================================
                                                       *ollama-common-options*
Common options ~



==============================================================================
                                                           *ollama-references*
Other references ~

                                                       *ollama-api-definition*
API definitions ~
	See https://github.com/jmorganca/ollama/blob/main/docs/api.md


==============================================================================
" vim:tw=78:ts=8:sw=8:ft=help:norl:noet:fen:
