*ollama.txt* A Vim plugin as ...
                                                  *ollama* *denops-ollama.txt*

Author: kyoh86 <me@kyoh86.dev>
License: MIT License


==============================================================================
                                                             *ollama-contents*
Contents ~

Install						|ollama-install|
Setup						|ollama-setup|
Function					|ollama-function|
Commands					|ollama-commands|
Keymaps						|ollama-keymaps|
Denops commands	 				|ollama-denops-commands|
Other references				|ollama-references|


==============================================================================
                                                              *ollama-install*
Install ~

You need to install *denops.vim* as a dependency.

* vim-denops/denops.vim https://github.com/vim-denops/denops.vim
* ollama https://ollama.ai


==============================================================================
                                                                *ollama-setup*
Setup ~


==============================================================================
                                                             *ollama-function*
Function ~

                                                         *ollama#open_log()*
ollama#open_log([{options}])
	Open Ollama log file.

	You can specify {options} with fields in below.
		opener	How the new buffer be placed in which window:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window


	Example: >
	:call ollama#open_log({'opener': 'new'})
<

                                                         *ollama#start_chat()*
ollama#start_chat({model} [, {options} [, {params}]])
	Start chat with Ollama using the {model}.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		opener	How the new buffer be placed in which window:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
		timeout	Time limit.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: >
	:call ollama#start_chat('codellama', {'opener': 'new'})
<

                                                           *ollama#complete()*
ollama#complete({model} [, {options} [, {params}]])
	Get completion for the current buffer around the cursor by the {model}.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		timeout	Time limit.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: >
	:call ollama#complete('codellama')
<

                                            *ollama#start_chat_with_context()*
ollama#start_chat_with_context({model}, {context} [, {options} [, {params}]])
	Start chat with Ollama with the context using the {model}.
	See |ollama-api-definition|.

	{context} specifies what kind of context should be passed to Ollama
	in advance.
	It supports the following entries:
		headMessage	A first message to be sent.
		selection	If the selection be sent.
		currentBuffer	If the current buffer be sent.
		buffers		A list of the bufinfo.
				It accepts buffer numbers (see |bufnr()|) or
			      	the objects from |getbufinro()|.
		lastMessage	A last message to be sent.

	You can specify {options} with fields in below.
		opener	How the new buffer be placed in which window:
			tabnew		(DEFAULT) open a new tab
			split, new	open a new split
			vsplit, vnew	open a new vertical split
			edit		open in the current window
		timeout	Time limit.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: >
	:call ollama#start_chat_with_context('codellama', {
		\ 'selection': v:true,
		\ 'buffers:' [1, {'bufnr': 2, 'name': 'foo'}],
		\ }, {'opener': 'tabnew'})
	:call ollama#start_chat_with_context('codellama', {
		\ 'selection': v:true,
		\ 'buffers:' getbufinfo({'buflisted':v:true}),
		\ })
<

                                                        *ollama#list_models()*
ollama#list_models([{options}])
	Show list models in local.

	You can specify {options} with fields in below.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	Example: >
	:call ollama#list_models()
<

                                                         *ollama#pull_model()*
ollama#pull_model({name} [, {options} [, {params}]])
	Pull a model by the {name} from the library.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		insecure	If it is true, allow insecure connections to
				the library.
		timeout		Time limit.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	And you can specify {params} as optional parameters for the API.
		See |ollama-api-definition|.

	Example: >
	:call ollama#pull_model('codellama')
<

                                                       *ollama#delete_model()*
ollama#delete_model({name} [, {options}])
	Delete a model by the {name} in local.
	See |ollama-api-definition|.

	You can specify {options} with fields in below.
		baseUrl	The base of the URL to request.
			(DEFAULT): "http://localhost:11434"

	Example: >
	:call ollama#delete_model('codellama')
<


==============================================================================
                                                             *ollama-commands*
Commands ~


==============================================================================
                                                              *ollama-keymaps*
Keymaps ~


==============================================================================
                                                       *ollama-common-options*
Common options ~



==============================================================================
                                                           *ollama-references*
Other references ~

                                                       *ollama-api-definition*
API definitions ~
	See https://github.com/jmorganca/ollama/blob/main/docs/api.md


==============================================================================
" vim:tw=78:ts=8:sw=8:ft=help:norl:noet:fen:
